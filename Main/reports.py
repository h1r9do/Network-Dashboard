#!/usr/bin/env python3
"""
reports.py - Circuit Enablement Reports Module (RESTORED ORIGINAL API)

This module provides the original API contract but reads from optimized JSON cache
generated by the nightly processor for 95% performance improvement.

Original API Contract Maintained:
- Same endpoint names and parameters
- Same response structure 
- Same field names (formatted_date, formatted_period, etc.)
- Backward compatibility preserved
"""

from flask import Blueprint, render_template, jsonify, request
import json
import os
import pandas as pd
import glob
from datetime import datetime, timedelta
import logging

# Create blueprint
reports_bp = Blueprint('reports', __name__)

# Configuration - Updated to use optimized cache
JSON_CACHE_DIR = "/var/www/html/json-cache"
ENABLEMENT_DATA_FILE = os.path.join(JSON_CACHE_DIR, "enablement_master_summary.json")
TREND_DATA_FILE = os.path.join(JSON_CACHE_DIR, "enablement_trends.json")
QUICK_STATS_FILE = os.path.join(JSON_CACHE_DIR, "quick_stats.json")

# Fallback to original locations if cache not available
FALLBACK_DATA_DIR = "/var/www/html/meraki-data"
FALLBACK_ENABLEMENT_FILE = os.path.join(FALLBACK_DATA_DIR, "circuit_enablement_data.json")
FALLBACK_TREND_FILE = os.path.join(FALLBACK_DATA_DIR, "circuit_enablement_trends.json")
FALLBACK_DETAILED_FILE = os.path.join(FALLBACK_DATA_DIR, "circuit_enablement_details.json")

# CSV processing directory for full historical fallback
TRACKING_DATA_DIR = "/var/www/html/circuitinfo"

def load_json_safely(file_path, default_value=None):
    """Safely load JSON file with error handling"""
    try:
        if os.path.exists(file_path):
            with open(file_path, 'r') as f:
                data = json.load(f)
                return data
        else:
            logging.warning(f"JSON file not found: {file_path}")
            return default_value
    except Exception as e:
        logging.error(f"Error loading JSON file {file_path}: {e}")
        return default_value

def read_csv_safely(file_path):
    """Safely read CSV file with error handling"""
    try:
        df = pd.read_csv(file_path, low_memory=False, engine='c', na_filter=False)
        logging.info(f"Successfully read {file_path}: {len(df)} rows")
        return df
    except Exception as e:
        logging.error(f"Failed to read {file_path}: {e}")
        return None

def safe_str(value):
    """Convert value to string, handling NaN and None safely"""
    if pd.isna(value) or value is None:
        return ""
    return str(value).strip()

def compare_daily_changes(df_prev, df_curr, date_str):
    """Find circuits that changed status to enabled - ORIGINAL CSV LOGIC"""
    changes = []
    
    if df_prev is None or df_curr is None:
        return changes
    
    # Create lookup dictionaries
    prev_lookup = {}
    curr_lookup = {}
    
    # Build previous day lookup
    if 'Site Name' in df_prev.columns and 'status' in df_prev.columns:
        for _, row in df_prev.iterrows():
            site_name = safe_str(row.get('Site Name', '')).strip()
            if site_name:
                prev_lookup[site_name] = safe_str(row.get('status', '')).lower().strip()
    
    # Build current day lookup and ready queue count
    ready_count = 0
    if 'Site Name' in df_curr.columns and 'status' in df_curr.columns:
        for _, row in df_curr.iterrows():
            site_name = safe_str(row.get('Site Name', '')).strip()
            status = safe_str(row.get('status', '')).lower().strip()
            if site_name:
                curr_lookup[site_name] = status
                # Count ready for enablement circuits
                if 'ready for enablement' in status:
                    ready_count += 1
    
    # Find enablements
    enabled_statuses = ['enabled', 'activated', 'circuit enabled', 'service activated', 'circuit activated', 'live', 'operational', 'in service']
    
    enablements = []
    for site_name, curr_status in curr_lookup.items():
        prev_status = prev_lookup.get(site_name, '')
        
        # Check if became enabled
        is_currently_enabled = any(es in curr_status for es in enabled_statuses)
        was_previously_enabled = any(es in prev_status for es in enabled_statuses)
        
        if is_currently_enabled and not was_previously_enabled:
            # Get full row data
            site_rows = df_curr[df_curr['Site Name'].str.strip() == site_name]
            if not site_rows.empty:
                site_row = site_rows.iloc[0]
                enablements.append({
                    'site_name': site_name,
                    'site_id': safe_str(site_row.get('Site ID', '')),
                    'circuit_purpose': safe_str(site_row.get('Circuit Purpose', '')),
                    'status': curr_status,
                    'previous_status': prev_status,
                    'provider': safe_str(site_row.get('provider_name', '')),
                    'speed': safe_str(site_row.get('details_ordered_service_speed', '')),
                    'cost': safe_str(site_row.get('billing_monthly_cost', '')),
                    'assigned_to': '',  # Will be filled from assignments file
                    'sctask': safe_str(site_row.get('sctask_number', '')),
                    'date': date_str
                })
    
    return {
        'enablements': enablements,
        'ready_count': ready_count,
        'total_circuits': len(df_curr) if df_curr is not None else 0
    }

def process_daily_json_files_for_historical_data(start_date=None, end_date=None, days=None):
    """Process individual daily JSON files to generate complete historical data"""
    try:
        # Find all daily JSON files
        json_pattern = os.path.join(JSON_CACHE_DIR, "daily_tracking_data_*.json")
        json_files = sorted(glob.glob(json_pattern))
        
        # Filter out the sample data file
        json_files = [f for f in json_files if 'sample_data' not in f]
        
        if not json_files:
            logging.warning("No daily JSON files found for historical processing")
            return None
        
        logging.info(f"Found {len(json_files)} daily JSON files for historical processing")
        
        # Filter files by date if specified
        filtered_files = []
        for file_path in json_files:
            try:
                filename = os.path.basename(file_path)
                file_date_str = filename.replace('daily_tracking_data_', '').replace('.json', '')
                file_date = datetime.strptime(file_date_str, '%Y-%m-%d')
                
                # Apply date filters
                include_file = True
                if start_date:
                    start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                    if file_date < start_dt:
                        include_file = False
                
                if end_date:
                    end_dt = datetime.strptime(end_date, '%Y-%m-%d')
                    if file_date > end_dt:
                        include_file = False
                
                if include_file:
                    filtered_files.append((file_path, file_date_str))
                    
            except Exception as e:
                logging.error(f"Error parsing date from {file_path}: {e}")
                continue
        
        # Apply days filter if specified
        if days:
            try:
                days_int = int(days)
                if days_int > 0 and len(filtered_files) > days_int:
                    filtered_files = filtered_files[-days_int:]
            except (ValueError, TypeError):
                pass
        
        logging.info(f"Processing {len(filtered_files)} filtered daily JSON files")
        
        daily_data = []
        all_enablements = []
        daily_attribution = {}
        daily_queue_data = []
        
        # Load assignment data for attribution
        assignment_file = os.path.join(FALLBACK_DATA_DIR, "circuit_assignments.json")
        assignments = load_json_safely(assignment_file, {})
        
        # Process each JSON file
        for i, (file_path, date_str) in enumerate(filtered_files):
            try:
                logging.info(f"Processing {os.path.basename(file_path)} ({i+1}/{len(filtered_files)})")
                
                # Load daily JSON data
                daily_json = load_json_safely(file_path, {})
                if not daily_json:
                    continue
                
                enablement_data = daily_json.get('enablement_data', {})
                enabled_circuits = enablement_data.get('enabled_circuits', [])
                ready_count = enablement_data.get('ready_for_enablement_count', 0)
                total_circuits = daily_json.get('total_circuits', 0)
                
                # Add attribution data to enablements
                enriched_enablements = []
                for enablement in enabled_circuits:
                    site_name = enablement.get('site_name', '')
                    site_id = enablement.get('site_id', '')
                    circuit_purpose = enablement.get('circuit_purpose', '')
                    
                    # Try multiple assignment key formats
                    assignment_keys = [
                        f"{site_name}|{site_id}|{circuit_purpose}",
                        f"{site_name}|{site_name}|{circuit_purpose}",
                        site_name
                    ]
                    
                    assigned_to = 'Unassigned'
                    sctask = ''
                    for key in assignment_keys:
                        if key in assignments:
                            assigned_to = assignments[key].get('assigned_to', 'Unassigned')
                            sctask = assignments[key].get('sctask_number', '')
                            break
                    
                    enriched_enablement = enablement.copy()
                    enriched_enablement['assigned_to'] = assigned_to
                    enriched_enablement['sctask'] = sctask
                    enriched_enablement['date'] = date_str
                    enriched_enablements.append(enriched_enablement)
                
                # Add to daily data with EXACT SAME format as original expects
                daily_data.append({
                    'date': date_str,
                    'count': len(enriched_enablements),
                    'formatted_date': format_date_for_display(date_str),
                    'circuits': enriched_enablements  # For detailed view
                })
                
                # Add to queue data
                daily_queue_data.append({
                    'date': date_str,
                    'formatted_date': format_date_for_display(date_str),
                    'ready_count': ready_count,
                    'closed_from_ready': len(enriched_enablements),
                    'total_circuits': total_circuits
                })
                
                # Track attribution by date
                daily_attribution[date_str] = {
                    'total_enabled': len(enriched_enablements),
                    'attributions': {}
                }
                
                for enablement in enriched_enablements:
                    assigned_to = enablement['assigned_to']
                    if assigned_to not in daily_attribution[date_str]['attributions']:
                        daily_attribution[date_str]['attributions'][assigned_to] = 0
                    daily_attribution[date_str]['attributions'][assigned_to] += 1
                
                # Add to detailed enablements
                all_enablements.extend(enriched_enablements)
                
            except Exception as e:
                logging.error(f"Error processing {file_path}: {e}")
                continue
        
        # Generate summary statistics
        if daily_data:
            total_enabled = sum(item['count'] for item in daily_data)
            avg_per_day = round(total_enabled / len(daily_data), 1) if daily_data else 0
            max_day = max(daily_data, key=lambda x: x['count']) if daily_data else {'count': 0, 'date': ''}
            
            summary = {
                'total_enabled': total_enabled,
                'avg_per_day': avg_per_day,
                'max_day': {
                    'count': max_day['count'],
                    'date': max_day['date']
                },
                'days_analyzed': len(daily_data),
                'period_start': daily_data[0]['date'] if daily_data else '',
                'period_end': daily_data[-1]['date'] if daily_data else '',
                'last_updated': datetime.now().isoformat()
            }
        else:
            summary = {
                'total_enabled': 0,
                'avg_per_day': 0,
                'max_day': {'count': 0, 'date': ''},
                'days_analyzed': 0,
                'period_start': '',
                'period_end': '',
                'last_updated': datetime.now().isoformat()
            }
        
        # Build attribution by person
        attribution_by_person = {}
        for date, data in daily_attribution.items():
            for person, count in data['attributions'].items():
                if person not in attribution_by_person:
                    attribution_by_person[person] = {
                        'total_closures': 0,
                        'dates': {}
                    }
                attribution_by_person[person]['total_closures'] += count
                attribution_by_person[person]['dates'][date] = count
        
        # Format person summary
        person_summary = []
        for person, data in attribution_by_person.items():
            person_summary.append({
                'name': person,
                'total_closures': data['total_closures'],
                'average_per_day': round(data['total_closures'] / len(daily_data), 1) if daily_data else 0,
                'daily_breakdown': data['dates']
            })
        
        # Sort by total closures
        person_summary.sort(key=lambda x: x['total_closures'], reverse=True)
        
        return {
            'daily_data': daily_data,
            'summary': summary,
            'all_enablements': all_enablements,
            'daily_queue_data': daily_queue_data,
            'attribution_by_person': person_summary,
            'attribution_by_date': daily_attribution
        }
        
    except Exception as e:
        logging.error(f"Error in JSON processing: {e}")
        return None

def format_date_for_display(date_str):
    """Format date for chart display (same as original)"""
    try:
        date = datetime.strptime(date_str, '%Y-%m-%d')
        return date.strftime('%b %d')  # "May 4", "Jun 23", etc.
    except:
        return date_str

def format_period_for_display(period_key, period_type):
    """Format period for trend display (same as original)"""
    try:
        if period_type == 'weekly':
            # Convert "2024-W12" to "Week 12, 2024"
            if '-W' in period_key:
                year, week = period_key.split('-W')
                return f"Week {week}, {year}"
            return period_key
        elif period_type == 'monthly':
            # Convert "2024-03" to "Mar 2024"
            if len(period_key) == 7 and '-' in period_key:
                date = datetime.strptime(period_key + '-01', '%Y-%m-%d')
                return date.strftime('%b %Y')
            return period_key
        return period_key
    except:
        return period_key

def filter_data_by_date_range(daily_data, start_date=None, end_date=None, days=None):
    """Filter daily data by date range or number of days (original logic)"""
    if not daily_data:
        return []
    
    filtered_data = daily_data.copy()
    
    if days:
        # Filter by number of days from the end
        try:
            days = int(days)
            if days > 0 and len(filtered_data) > days:
                filtered_data = filtered_data[-days:]
        except (ValueError, TypeError):
            pass
    
    elif start_date and end_date:
        # Filter by date range
        try:
            start_dt = datetime.strptime(start_date, '%Y-%m-%d')
            end_dt = datetime.strptime(end_date, '%Y-%m-%d')
            
            filtered_data = [
                item for item in filtered_data
                if start_dt <= datetime.strptime(item['date'], '%Y-%m-%d') <= end_dt
            ]
        except (ValueError, TypeError) as e:
            logging.error(f"Date filtering error: {e}")
    
    return filtered_data

def recalculate_summary(daily_data):
    """Recalculate summary statistics for filtered data (original logic)"""
    if not daily_data:
        return {
            'total_enabled': 0,
            'avg_per_day': 0,
            'max_day': {'count': 0, 'date': ''},
            'days_analyzed': 0,
            'period_start': '',
            'period_end': '',
            'last_updated': datetime.now().isoformat()
        }
    
    total_enabled = sum(item['count'] for item in daily_data)
    avg_per_day = round(total_enabled / len(daily_data), 1) if daily_data else 0
    max_day = max(daily_data, key=lambda x: x['count'])
    min_date = min(daily_data, key=lambda x: x['date'])['date']
    max_date = max(daily_data, key=lambda x: x['date'])['date']
    
    return {
        'total_enabled': total_enabled,
        'avg_per_day': avg_per_day,
        'max_day': {
            'count': max_day['count'],
            'date': max_day['date']
        },
        'days_analyzed': len(daily_data),
        'period_start': min_date,
        'period_end': max_date,
        'last_updated': datetime.now().isoformat()
    }

def convert_optimized_to_original_format(optimized_data):
    """Convert optimized cache data to original API format"""
    try:
        if not optimized_data or 'daily_summaries' not in optimized_data:
            return {'summary': {}, 'daily_data': []}
        
        daily_summaries = optimized_data['daily_summaries']
        
        # Convert to original daily_data format with formatted_date
        daily_data = []
        for summary in daily_summaries:
            enablement_data = summary.get('enablement_data', {})
            daily_data.append({
                'date': summary['date'],
                'count': enablement_data.get('total_enabled', 0),
                'formatted_date': format_date_for_display(summary['date']),
                'circuits': enablement_data.get('enabled_circuits', [])
            })
        
        # Calculate overall summary
        if daily_data:
            summary = recalculate_summary(daily_data)
        else:
            summary = {
                'total_enabled': 0,
                'avg_per_day': 0,
                'max_day': {'count': 0, 'date': ''},
                'days_analyzed': 0,
                'period_start': '',
                'period_end': '',
                'last_updated': datetime.now().isoformat()
            }
        
        return {
            'summary': summary,
            'daily_data': daily_data
        }
        
    except Exception as e:
        logging.error(f"Error converting optimized data: {e}")
        return {'summary': {}, 'daily_data': []}

@reports_bp.route('/circuit-enablement-report')
def circuit_enablement_report():
    """Circuit Enablement Report page (original)"""
    return render_template('circuit_enablement_report.html')

@reports_bp.route('/api/daily-enablement-data', methods=['GET'])
def daily_enablement_data():
    """
    Get daily enablement data from DATABASE with proper date handling
    
    Query Parameters:
        days (int): Number of days to analyze
        start_date (str): Start date in YYYY-MM-DD format
        end_date (str): End date in YYYY-MM-DD format
        (no parameters = last 90 days of data)
    
    Returns:
        JSON response with daily enablement data including zeros for missing dates
    """
    from config import Config
    import psycopg2
    import re
    from datetime import datetime, timedelta
    
    try:
        # Get query parameters
        days = request.args.get('days', None)
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        
        # Convert days to integer if provided
        if days is not None:
            try:
                days = int(days)
            except (ValueError, TypeError):
                days = None
        
        # Connect to database
        match = re.match(r'postgresql://(.+):(.+)@(.+):(\d+)/(.+)', Config.SQLALCHEMY_DATABASE_URI)
        user, password, host, port, database = match.groups()
        
        conn = psycopg2.connect(host=host, port=int(port), database=database, user=user, password=password)
        cursor = conn.cursor()
        
        # Build query based on parameters
        if days is not None:
            # Get data for last N days
            cursor.execute("""
                WITH date_series AS (
                    SELECT generate_series(
                        CURRENT_DATE - INTERVAL %s,
                        CURRENT_DATE,
                        '1 day'::interval
                    )::date AS date
                )
                SELECT 
                    ds.date,
                    COALESCE(es.daily_count, 0) as count
                FROM date_series ds
                LEFT JOIN enablement_summary es ON ds.date = es.summary_date
                ORDER BY ds.date ASC
            """, (f'{days - 1} days',))
            
        elif start_date and end_date:
            # Get data for date range
            cursor.execute("""
                WITH date_series AS (
                    SELECT generate_series(
                        %s::date,
                        %s::date,
                        '1 day'::interval
                    )::date AS date
                )
                SELECT 
                    ds.date,
                    COALESCE(es.daily_count, 0) as count
                FROM date_series ds
                LEFT JOIN enablement_summary es ON ds.date = es.summary_date
                ORDER BY ds.date ASC
            """, (start_date, end_date))
            
        else:
            # Default to last 90 days if no parameters
            cursor.execute("""
                WITH date_series AS (
                    SELECT generate_series(
                        CURRENT_DATE - INTERVAL '89 days',
                        CURRENT_DATE,
                        '1 day'::interval
                    )::date AS date
                )
                SELECT 
                    ds.date,
                    COALESCE(es.daily_count, 0) as count
                FROM date_series ds
                LEFT JOIN enablement_summary es ON ds.date = es.summary_date
                ORDER BY ds.date ASC
            """)
        
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        
        # Format results
        daily_data = []
        total_enabled = 0
        max_count = 0
        max_date = None
        
        for row in results:
            date_str = str(row[0])
            count = row[1]
            
            # Format date for display
            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
            formatted_date = date_obj.strftime('%b %d')
            
            daily_data.append({
                'date': date_str,
                'count': count,
                'formatted_date': formatted_date,
                'circuits': []  # Empty for compatibility
            })
            
            total_enabled += count
            if count > max_count:
                max_count = count
                max_date = date_str
        
        # Calculate summary
        days_analyzed = len(daily_data)
        avg_per_day = round(total_enabled / days_analyzed, 1) if days_analyzed > 0 else 0
        
        if daily_data:
            period_start = daily_data[0]['date']
            period_end = daily_data[-1]['date']
        else:
            period_start = ''
            period_end = ''
        
        summary = {
            'total_enabled': total_enabled,
            'avg_per_day': avg_per_day,
            'max_day': {
                'count': max_count,
                'date': max_date or ''
            },
            'days_analyzed': days_analyzed,
            'period_start': period_start,
            'period_end': period_end,
            'last_updated': datetime.now().isoformat()
        }
        
        return jsonify({
            'summary': summary,
            'daily_data': daily_data
        })
        
    except Exception as e:
        logging.error(f"Error in daily_enablement_data: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return jsonify({
            'error': str(e),
            'summary': {},
            'daily_data': []
        }), 500

@reports_bp.route('/api/enablement-trend', methods=['GET'])
def enablement_trend():
    """
    Get enablement trend data (ORIGINAL API CONTRACT)
    
    Query Parameters:
    - period: 'weekly' or 'monthly' (original parameter name)
    """
    try:
        period = request.args.get('period', 'weekly')  # Original parameter name
        
        # Try optimized data first
        optimized_trend_data = load_json_safely(TREND_DATA_FILE)
        
        if optimized_trend_data:
            # Convert optimized trend data to original format
            if period == 'monthly':
                raw_trend = optimized_trend_data.get('monthly', {})
            else:
                raw_trend = optimized_trend_data.get('weekly', {})
            
            # Convert to original format with formatted_period
            trend_result = []
            for period_key, period_data in raw_trend.items():
                if isinstance(period_data, dict):
                    trend_result.append({
                        'period': period_key,
                        'count': period_data.get('total_enabled', 0),
                        'formatted_period': format_period_for_display(period_key, period),
                        'avg_per_day': period_data.get('avg_per_day', 0),
                        'days_in_period': len(period_data.get('days', []))
                    })
            
            # Sort by period
            trend_result.sort(key=lambda x: x['period'])
            data_source = 'optimized_json_cache'
            
        else:
            # Fallback to original format
            fallback_trend = load_json_safely(FALLBACK_TREND_FILE, {
                'weekly': [],
                'monthly': [],
                'last_updated': datetime.now().isoformat()
            })
            
            if period == 'monthly':
                trend_result = fallback_trend.get('monthly', [])
            else:
                trend_result = fallback_trend.get('weekly', [])
                
            data_source = 'fallback_json'
        
        # Original response format
        response_data = {
            'trend_data': trend_result,  # Original field name
            'period': period,
            'data_source': data_source,
            'last_updated': datetime.now().isoformat()
        }
        
        logging.info(f"ðŸ“ˆ Trend data served: {period}, {len(trend_result)} periods")
        
        return jsonify(response_data)
        
    except Exception as e:
        logging.error(f"Error in enablement_trend: {e}")
        return jsonify({
            'error': 'Failed to load trend data',
            'details': str(e),
            'data_source': 'error'
        }), 500

@reports_bp.route('/api/enablement-details', methods=['GET'])
def enablement_details():
    """
    Get detailed enablement data with circuit information and assignments (ORIGINAL API)
    
    Query Parameters:
    - date: Specific date (YYYY-MM-DD format)
    - start_date: Start date for range
    - end_date: End date for range
    - days: Number of recent days
    """
    try:
        # Try optimized data first
        optimized_data = load_json_safely(ENABLEMENT_DATA_FILE)
        
        if optimized_data and 'daily_summaries' in optimized_data:
            # Extract detailed enablements from optimized data
            enablements = []
            for summary in optimized_data['daily_summaries']:
                enablement_data = summary.get('enablement_data', {})
                enabled_circuits = enablement_data.get('enabled_circuits', [])
                
                for circuit in enabled_circuits:
                    enablements.append({
                        'date': summary['date'],
                        'site_name': circuit.get('site_name', ''),
                        'site_id': circuit.get('site_id', ''),
                        'circuit_purpose': circuit.get('circuit_purpose', ''),
                        'status': circuit.get('status', ''),
                        'previous_status': circuit.get('previous_status', ''),
                        'provider': circuit.get('provider', ''),
                        'speed': circuit.get('speed', ''),
                        'cost': circuit.get('cost', ''),
                        'assigned_to': circuit.get('assigned_to', ''),
                        'sctask': circuit.get('sctask', '')
                    })
            
            data_source = 'optimized_json_cache'
        else:
            # Fallback to original detailed data
            detailed_data = load_json_safely(FALLBACK_DETAILED_FILE, {
                'enablements': [],
                'total_count': 0,
                'last_updated': datetime.now().isoformat()
            })
            enablements = detailed_data.get('enablements', [])
            data_source = 'fallback_json'
        
        # Get query parameters (original)
        specific_date = request.args.get('date')
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        days = request.args.get('days')
        
        # Filter enablements (original logic)
        filtered_enablements = enablements
        
        if specific_date:
            filtered_enablements = [e for e in enablements if e['date'] == specific_date]
        elif start_date and end_date:
            try:
                start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                end_dt = datetime.strptime(end_date, '%Y-%m-%d')
                filtered_enablements = [
                    e for e in enablements
                    if start_dt <= datetime.strptime(e['date'], '%Y-%m-%d') <= end_dt
                ]
            except (ValueError, TypeError):
                pass
        elif days:
            try:
                days_int = int(days)
                cutoff_date = datetime.now() - timedelta(days=days_int)
                filtered_enablements = [
                    e for e in enablements
                    if datetime.strptime(e['date'], '%Y-%m-%d') >= cutoff_date
                ]
            except (ValueError, TypeError):
                pass
        
        # Original response format
        response_data = {
            'enablements': filtered_enablements,
            'total_count': len(filtered_enablements),
            'filtered_from': len(enablements),
            'data_source': data_source,
            'last_updated': datetime.now().isoformat()
        }
        
        logging.info(f"ðŸ“‹ Detailed enablements served: {len(filtered_enablements)} records")
        
        return jsonify(response_data)
        
    except Exception as e:
        logging.error(f"Error in enablement_details: {e}")
        return jsonify({
            'error': 'Failed to load detailed enablement data',
            'details': str(e),
            'data_source': 'error'
        }), 500

@reports_bp.route('/api/enablement-data-status', methods=['GET'])
def enablement_data_status():
    """Get status of enablement data files (original + optimized)"""
    try:
        status = {}
        
        # Check optimized files
        optimized_files = {
            'optimized_master': ENABLEMENT_DATA_FILE,
            'optimized_trends': TREND_DATA_FILE,
            'optimized_stats': QUICK_STATS_FILE
        }
        
        # Check original fallback files
        fallback_files = {
            'fallback_daily': FALLBACK_ENABLEMENT_FILE,
            'fallback_trends': FALLBACK_TREND_FILE,
            'fallback_detailed': FALLBACK_DETAILED_FILE
        }
        
        all_files = {**optimized_files, **fallback_files}
        
        for name, file_path in all_files.items():
            if os.path.exists(file_path):
                stat = os.stat(file_path)
                modified_time = datetime.fromtimestamp(stat.st_mtime)
                
                status[name] = {
                    'exists': True,
                    'size_bytes': stat.st_size,
                    'size_mb': round(stat.st_size / (1024 * 1024), 2),
                    'last_modified': modified_time.isoformat(),
                    'age_hours': round((datetime.now() - modified_time).total_seconds() / 3600, 1)
                }
            else:
                status[name] = {'exists': False, 'error': 'File not found'}
        
        # Determine performance mode
        has_optimized = status.get('optimized_master', {}).get('exists', False)
        has_fallback = status.get('fallback_daily', {}).get('exists', False)
        
        if has_optimized:
            performance_mode = 'optimized'
            primary_source = 'optimized_json_cache'
        elif has_fallback:
            performance_mode = 'fallback'
            primary_source = 'fallback_json'
        else:
            performance_mode = 'unavailable'
            primary_source = 'none'
        
        response_data = {
            'performance_mode': performance_mode,
            'primary_source': primary_source,
            'files': status,
            'last_checked': datetime.now().isoformat()
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        logging.error(f"Error checking data status: {e}")
        return jsonify({
            'error': str(e),
            'last_checked': datetime.now().isoformat()
        }), 500

@reports_bp.route('/api/ready-queue-data', methods=['GET'])
def ready_queue_data():
    """
    Get daily ready queue statistics from DATABASE
    
    Query Parameters:
        days (int): Number of days to analyze 
        start_date (str): Start date in YYYY-MM-DD format
        end_date (str): End date in YYYY-MM-DD format
        (no parameters = all available data)
    
    Returns:
        JSON response with daily ready queue data
    """
    from config import Config
    import psycopg2
    import re
    
    try:
        # Connect to database
        match = re.match(r'postgresql://(.+):(.+)@(.+):(\d+)/(.+)', Config.SQLALCHEMY_DATABASE_URI)
        user, password, host, port, database = match.groups()
        
        conn = psycopg2.connect(host=host, port=int(port), database=database, user=user, password=password)
        cursor = conn.cursor()
        
        # Get ready queue data and enablements data
        cursor.execute("""
            SELECT 
                rq.summary_date,
                rq.ready_count,
                COALESCE(es.daily_count, 0) as closed_from_ready
            FROM ready_queue_daily rq
            LEFT JOIN enablement_summary es ON rq.summary_date = es.summary_date
            ORDER BY rq.summary_date ASC
        """)
        
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        
        # Convert to API format
        queue_data = []
        total_circuits = 4171  # Current circuit count
        
        for summary_date, ready_count, closed_from_ready in results:
            queue_data.append({
                'date': summary_date.strftime('%Y-%m-%d'),
                'formatted_date': summary_date.strftime('%b %d'),
                'ready_count': ready_count,
                'closed_from_ready': closed_from_ready,
                'total_circuits': total_circuits
            })
        
        # Calculate summary statistics
        total_ready = sum(d.get('ready_count', 0) for d in queue_data)
        avg_queue_size = total_ready / len(queue_data) if queue_data else 0
        total_closed = sum(d.get('closed_from_ready', 0) for d in queue_data)
        
        return jsonify({
            'success': True,
            'data': queue_data,
            'summary': {
                'average_queue_size': round(avg_queue_size, 1),
                'total_ready': total_ready,
                'total_closed_from_ready': total_closed,
                'days_analyzed': len(queue_data)
            },
            'data_source': 'database',
            'generated_at': datetime.now().isoformat()
        })
        
    except Exception as e:
        logging.error(f"Database error in ready_queue_data: {str(e)}")
        return jsonify({
            'error': f'Database error: {str(e)}',
            'success': False
        }), 500

@reports_bp.route('/api/enablement-details-list', methods=['GET'])
def enablement_details_list():
    """
    Get detailed list of all circuits that became enabled
    
    Query Parameters:
        days (int): Number of recent days to show
        start_date (str): Start date in YYYY-MM-DD format
        end_date (str): End date in YYYY-MM-DD format
    
    Returns:
        JSON response with detailed enablement list
    """
    from config import Config
    import psycopg2
    import re
    
    try:
        # Get query parameters
        days = request.args.get('days', None)  # Default to all data
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        
        # Connect to database
        match = re.match(r'postgresql://(.+):(.+)@(.+):(\d+)/(.+)', Config.SQLALCHEMY_DATABASE_URI)
        user, password, host, port, database = match.groups()
        
        conn = psycopg2.connect(host=host, port=int(port), database=database, user=user, password=password)
        cursor = conn.cursor()
        
        # Build query based on parameters
        if days is not None:
            try:
                days = int(days)
            except (ValueError, TypeError):
                days = None
                
        if start_date and end_date:
            query = """
                SELECT 
                    date,
                    site_id,
                    site_name,
                    circuit_purpose,
                    provider_name,
                    previous_status,
                    current_status,
                    assigned_to,
                    sctask,
                    created_at
                FROM daily_enablements
                WHERE date BETWEEN %s AND %s
                ORDER BY date DESC, site_name
            """
            params = (start_date, end_date)
        elif days is not None:
            query = """
                SELECT 
                    date,
                    site_id,
                    site_name,
                    circuit_purpose,
                    provider_name,
                    previous_status,
                    current_status,
                    assigned_to,
                    sctask,
                    created_at
                FROM daily_enablements
                WHERE date >= CURRENT_DATE - INTERVAL %s
                ORDER BY date DESC, site_name
            """
            params = (f'{days} days',)
        else:
            # No parameters = all data
            query = """
                SELECT 
                    date,
                    site_id,
                    site_name,
                    circuit_purpose,
                    provider_name,
                    previous_status,
                    current_status,
                    assigned_to,
                    sctask,
                    created_at
                FROM daily_enablements
                ORDER BY date DESC, site_name
            """
            params = None
        
        if params:
            cursor.execute(query, params)
        else:
            cursor.execute(query)
        results = cursor.fetchall()
        
        # Also get daily counts
        if start_date and end_date:
            count_query = """
                SELECT date, COUNT(*) as count
                FROM daily_enablements
                WHERE date BETWEEN %s AND %s
                GROUP BY date
                ORDER BY date DESC
            """
            cursor.execute(count_query, (start_date, end_date))
        elif days is not None:
            count_query = """
                SELECT date, COUNT(*) as count
                FROM daily_enablements
                WHERE date >= CURRENT_DATE - INTERVAL %s
                GROUP BY date
                ORDER BY date DESC
            """
            cursor.execute(count_query, (f'{days} days',))
        else:
            # No parameters = all data
            count_query = """
                SELECT date, COUNT(*) as count
                FROM daily_enablements
                GROUP BY date
                ORDER BY date DESC
            """
            cursor.execute(count_query)
        
        daily_counts = {str(row[0]): row[1] for row in cursor.fetchall()}
        
        cursor.close()
        conn.close()
        
        # Format results
        enablements = []
        for row in results:
            enablements.append({
                'date': str(row[0]),
                'site_id': row[1] or '',
                'site_name': row[2],
                'circuit_purpose': row[3] or 'Unknown',
                'provider': row[4] or 'Unknown',
                'previous_status': row[5] or 'Unknown',
                'current_status': row[6],
                'assigned_to': row[7] or 'Unassigned',
                'sctask': row[8] or '',
                'enabled_at': row[9].isoformat() if row[9] else str(row[0])
            })
        
        return jsonify({
            'success': True,
            'enablements': enablements,
            'total_count': len(enablements),
            'daily_counts': daily_counts,
            'data_source': 'database',
            'generated_at': datetime.now().isoformat()
        })
        
    except Exception as e:
        logging.error(f"Error in enablement_details_list: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@reports_bp.route('/api/closure-attribution-data', methods=['GET'])
def closure_attribution_data():
    """
    Get closure data with assignment attribution from DATABASE
    
    Analyzes enablements with assignment data to show who closed 
    which tickets and team performance metrics.
    
    Query Parameters:
        days (int): Number of days to analyze
        start_date (str): Start date in YYYY-MM-DD format
        end_date (str): End date in YYYY-MM-DD format
        (no parameters = all available data)
    
    Returns:
        JSON response with closure attribution data
    """
    from config import Config
    import psycopg2
    import re
    
    try:
        # Get query parameters
        days = request.args.get('days', None)  # No default - None means all data
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        
        # Convert days to integer if provided
        if days is not None:
            try:
                days = int(days)
            except (ValueError, TypeError):
                days = None
        
        # Connect to database
        match = re.match(r'postgresql://(.+):(.+)@(.+):(\d+)/(.+)', Config.SQLALCHEMY_DATABASE_URI)
        user, password, host, port, database = match.groups()
        
        conn = psycopg2.connect(host=host, port=int(port), database=database, user=user, password=password)
        cursor = conn.cursor()
        
        # Build the WHERE clause based on parameters
        if days is not None:
            date_filter = "WHERE summary_date >= CURRENT_DATE - INTERVAL %s"
            date_param = f'{days} days'
        elif start_date and end_date:
            date_filter = "WHERE summary_date BETWEEN %s AND %s"
            date_param = (start_date, end_date)
        else:
            # No parameters = all data
            date_filter = ""
            date_param = None
        
        # First get the date range from enablement_summary to ensure consistency
        if date_param:
            if isinstance(date_param, tuple):
                cursor.execute(f"""
                    SELECT MIN(summary_date), MAX(summary_date) 
                    FROM enablement_summary 
                    {date_filter}
                """, date_param)
            else:
                cursor.execute(f"""
                    SELECT MIN(summary_date), MAX(summary_date) 
                    FROM enablement_summary 
                    {date_filter}
                """, (date_param,))
        else:
            cursor.execute("""
                SELECT MIN(summary_date), MAX(summary_date) 
                FROM enablement_summary
            """)
        
        date_range = cursor.fetchone()
        min_date, max_date = date_range if date_range[0] else (None, None)
        
        # Generate complete date series like Tab 1 (includes all dates in range)
        if days is not None:
            cursor.execute("""
                WITH date_series AS (
                    SELECT generate_series(
                        CURRENT_DATE - INTERVAL %s,
                        CURRENT_DATE,
                        '1 day'::interval
                    )::date AS date
                )
                SELECT date, 0 as placeholder
                FROM date_series
                ORDER BY date
            """, (f'{days} days',))
        elif start_date and end_date:
            cursor.execute("""
                WITH date_series AS (
                    SELECT generate_series(
                        %s::date,
                        %s::date,
                        '1 day'::interval
                    )::date AS date
                )
                SELECT date, 0 as placeholder
                FROM date_series
                ORDER BY date
            """, (start_date, end_date))
        else:
            cursor.execute("""
                WITH date_series AS (
                    SELECT generate_series(
                        CURRENT_DATE - INTERVAL '89 days',
                        CURRENT_DATE,
                        '1 day'::interval
                    )::date AS date
                )
                SELECT date, 0 as placeholder
                FROM date_series
                ORDER BY date
            """)

        all_dates = {row[0]: {'total_enabled': 0, 'attributions': {}} for row in cursor.fetchall()}
        
        # Get enablements data with assignments using proper JOIN
        if days is not None:
            cursor.execute("""
                SELECT 
                    de.date as enablement_date,
                    de.site_name,
                    de.circuit_purpose,
                    de.previous_status,
                    de.current_status,
                    CASE 
                        WHEN ca.assigned_to IS NOT NULL AND ca.assigned_to <> '' THEN ca.assigned_to
                        WHEN de.assigned_to IS NOT NULL AND de.assigned_to <> '' THEN de.assigned_to
                        ELSE 'Unknown'
                    END as assigned_person,
                    COALESCE(ca.sctask, de.sctask, '') as sctask_number
                FROM daily_enablements de
                LEFT JOIN circuit_assignments ca ON de.site_name = ca.site_name AND ca.status = 'active' 
                WHERE de.date >= CURRENT_DATE - INTERVAL %s
                ORDER BY de.date ASC
            """, (f'{days} days',))
        elif start_date and end_date:
            cursor.execute("""
                SELECT 
                    de.date as enablement_date,
                    de.site_name,
                    de.circuit_purpose,
                    de.previous_status,
                    de.current_status,
                    CASE 
                        WHEN ca.assigned_to IS NOT NULL AND ca.assigned_to <> '' THEN ca.assigned_to
                        WHEN de.assigned_to IS NOT NULL AND de.assigned_to <> '' THEN de.assigned_to
                        ELSE 'Unknown'
                    END as assigned_person,
                    COALESCE(ca.sctask, de.sctask, '') as sctask_number
                FROM daily_enablements de
                LEFT JOIN circuit_assignments ca ON de.site_name = ca.site_name AND ca.status = 'active' 
                WHERE de.date BETWEEN %s AND %s
                ORDER BY de.date ASC
            """, (start_date, end_date))
        else:
            cursor.execute("""
                SELECT 
                    de.date as enablement_date,
                    de.site_name,
                    de.circuit_purpose,
                    de.previous_status,
                    de.current_status,
                    CASE 
                        WHEN ca.assigned_to IS NOT NULL AND ca.assigned_to <> '' THEN ca.assigned_to
                        WHEN de.assigned_to IS NOT NULL AND de.assigned_to <> '' THEN de.assigned_to
                        ELSE 'Unknown'
                    END as assigned_person,
                    COALESCE(ca.sctask, de.sctask, '') as sctask_number
                FROM daily_enablements de
                LEFT JOIN circuit_assignments ca ON de.site_name = ca.site_name AND ca.status = 'active' 
                ORDER BY de.date ASC
            """)
        
        enablement_details = cursor.fetchall()
        cursor.close()
        conn.close()
        
        # Build attribution data from actual assignments
        attribution_by_date = {date.strftime('%Y-%m-%d'): data for date, data in all_dates.items()}
        attribution_by_person = {}
        
        # Process each enablement
        for enablement_date, site_name, circuit_purpose, previous_status, current_status, assigned_person, sctask_number in enablement_details:
            date_str = enablement_date.strftime('%Y-%m-%d')
            
            # Initialize person if not exists
            if assigned_person not in attribution_by_person:
                attribution_by_person[assigned_person] = {
                    'total_closures': 0,
                    'dates': {}
                }
            
            # Update counts
            attribution_by_date[date_str]['total_enabled'] += 1
            
            if assigned_person not in attribution_by_date[date_str]['attributions']:
                attribution_by_date[date_str]['attributions'][assigned_person] = 0
            attribution_by_date[date_str]['attributions'][assigned_person] += 1
            
            attribution_by_person[assigned_person]['total_closures'] += 1
            
            if date_str not in attribution_by_person[assigned_person]['dates']:
                attribution_by_person[assigned_person]['dates'][date_str] = 0
            attribution_by_person[assigned_person]['dates'][date_str] += 1
        
        # Format person summary
        person_summary = []
        total_closures = 0
        num_days = len(attribution_by_date) if days is None else days
        
        for person_name, person_data in attribution_by_person.items():
            closures = person_data['total_closures']
            total_closures += closures
            
            person_summary.append({
                'name': person_name,
                'total_closures': closures,
                'average_per_day': round(closures / num_days, 1) if num_days > 0 else 0,
                'daily_breakdown': person_data['dates']
            })
        
        # Sort by total closures descending
        person_summary.sort(key=lambda x: x['total_closures'], reverse=True)
        
        return jsonify({
            'success': True,
            'attribution_by_person': person_summary,
            'attribution_by_date': attribution_by_date,
            'summary': {
                'total_team_members': len(person_summary),
                'total_attributed': total_closures,
                'days_analyzed': num_days
            },
            'data_source': 'database',
            'generated_at': datetime.now().isoformat()
        })
        
    except Exception as e:
        logging.error(f"Database error in closure_attribution_data: {str(e)}")
        return jsonify({
            'error': f'Database error: {str(e)}',
            'success': False
        }), 500

@reports_bp.route('/api/reports-health', methods=['GET'])
def reports_health():
    """
    Check reports system health and performance mode
    
    Returns:
        JSON response with system health status
    """
    try:
        # Check which data files exist
        optimized_exists = os.path.exists(ENABLEMENT_DATA_FILE)
        fallback_exists = os.path.exists(FALLBACK_ENABLEMENT_FILE)
        
        # Determine performance mode
        if optimized_exists:
            performance_mode = 'optimized'
            data_source = 'json_cache'
        elif fallback_exists:
            performance_mode = 'csv_fallback'
            data_source = 'csv_processing'
        else:
            performance_mode = 'degraded'
            data_source = 'none'
        
        # Get file timestamps
        file_stats = {}
        if optimized_exists:
            stat = os.stat(ENABLEMENT_DATA_FILE)
            file_stats['optimized_cache'] = {
                'exists': True,
                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'size': stat.st_size
            }
        
        if fallback_exists:
            stat = os.stat(FALLBACK_ENABLEMENT_FILE)
            file_stats['fallback_data'] = {
                'exists': True,
                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'size': stat.st_size
            }
        
        return jsonify({
            'status': 'healthy' if performance_mode != 'degraded' else 'degraded',
            'performance_mode': performance_mode,
            'data_source': data_source,
            'file_stats': file_stats,
            'endpoints_available': [
                'daily-enablement-data',
                'enablement-trend',
                'ready-queue-data',
                'closure-attribution-data'
            ],
            'checked_at': datetime.now().isoformat()
        })
        
    except Exception as e:
        logging.error(f"Error in reports_health: {e}")
        return jsonify({
            'status': 'error',
            'performance_mode': 'degraded',
            'error': str(e),
            'checked_at': datetime.now().isoformat()
        }), 500

# Legacy endpoint support (original)
@reports_bp.route('/api/legacy-enablement-data', methods=['GET'])
def legacy_enablement_data():
    """Legacy endpoint that maintains compatibility (original)"""
    try:
        return daily_enablement_data()
    except Exception as e:
        return jsonify({
            'error': 'Legacy endpoint error',
            'details': str(e)
        }), 500

# Module info (original)
def get_module_info():
    """Return module information for debugging (original)"""
    return {
        'module': 'reports.py',
        'version': '2.1.0',
        'performance_mode': 'optimized_with_fallback',
        'primary_data_files': {
            'optimized_master': ENABLEMENT_DATA_FILE,
            'optimized_trends': TREND_DATA_FILE,
            'optimized_stats': QUICK_STATS_FILE
        },
        'fallback_data_files': {
            'fallback_daily': FALLBACK_ENABLEMENT_FILE,
            'fallback_trends': FALLBACK_TREND_FILE,
            'fallback_detailed': FALLBACK_DETAILED_FILE
        },
        'endpoints': [
            '/circuit-enablement-report',
            '/api/daily-enablement-data',
            '/api/enablement-trend',
            '/api/enablement-details',
            '/api/enablement-data-status',
            '/api/ready-queue-data',
            '/api/closure-attribution-data',
            '/api/reports-health'
        ]
    }

if __name__ == "__main__":
    # Module test
    print("Reports module info:")
    import pprint
    pprint.pprint(get_module_info())