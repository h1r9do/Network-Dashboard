#!/usr/bin/env python3
"""
reports.py - Circuit Enablement Reports Module (RESTORED ORIGINAL API)

This module provides the original API contract but reads from optimized JSON cache
generated by the nightly processor for 95% performance improvement.

Original API Contract Maintained:
- Same endpoint names and parameters
- Same response structure 
- Same field names (formatted_date, formatted_period, etc.)
- Backward compatibility preserved
"""

from flask import Blueprint, render_template, jsonify, request
import json
import os
import pandas as pd
import glob
from datetime import datetime, timedelta
import logging

# Create blueprint
reports_bp = Blueprint('reports', __name__)

# Configuration - Updated to use optimized cache
JSON_CACHE_DIR = "/var/www/html/json-cache"
ENABLEMENT_DATA_FILE = os.path.join(JSON_CACHE_DIR, "enablement_master_summary.json")
TREND_DATA_FILE = os.path.join(JSON_CACHE_DIR, "enablement_trends.json")
QUICK_STATS_FILE = os.path.join(JSON_CACHE_DIR, "quick_stats.json")

# Fallback to original locations if cache not available
FALLBACK_DATA_DIR = "/var/www/html/meraki-data"
FALLBACK_ENABLEMENT_FILE = os.path.join(FALLBACK_DATA_DIR, "circuit_enablement_data.json")
FALLBACK_TREND_FILE = os.path.join(FALLBACK_DATA_DIR, "circuit_enablement_trends.json")
FALLBACK_DETAILED_FILE = os.path.join(FALLBACK_DATA_DIR, "circuit_enablement_details.json")

# CSV processing directory for full historical fallback
TRACKING_DATA_DIR = "/var/www/html/circuitinfo"

def load_json_safely(file_path, default_value=None):
    """Safely load JSON file with error handling"""
    try:
        if os.path.exists(file_path):
            with open(file_path, 'r') as f:
                data = json.load(f)
                return data
        else:
            logging.warning(f"JSON file not found: {file_path}")
            return default_value
    except Exception as e:
        logging.error(f"Error loading JSON file {file_path}: {e}")
        return default_value

def read_csv_safely(file_path):
    """Safely read CSV file with error handling"""
    try:
        df = pd.read_csv(file_path, low_memory=False, engine='c', na_filter=False)
        logging.info(f"Successfully read {file_path}: {len(df)} rows")
        return df
    except Exception as e:
        logging.error(f"Failed to read {file_path}: {e}")
        return None

def safe_str(value):
    """Convert value to string, handling NaN and None safely"""
    if pd.isna(value) or value is None:
        return ""
    return str(value).strip()

def compare_daily_changes(df_prev, df_curr, date_str):
    """Find circuits that changed status to enabled - ORIGINAL CSV LOGIC"""
    changes = []
    
    if df_prev is None or df_curr is None:
        return changes
    
    # Create lookup dictionaries
    prev_lookup = {}
    curr_lookup = {}
    
    # Build previous day lookup
    if 'Site Name' in df_prev.columns and 'status' in df_prev.columns:
        for _, row in df_prev.iterrows():
            site_name = safe_str(row.get('Site Name', '')).strip()
            if site_name:
                prev_lookup[site_name] = safe_str(row.get('status', '')).lower().strip()
    
    # Build current day lookup and ready queue count
    ready_count = 0
    if 'Site Name' in df_curr.columns and 'status' in df_curr.columns:
        for _, row in df_curr.iterrows():
            site_name = safe_str(row.get('Site Name', '')).strip()
            status = safe_str(row.get('status', '')).lower().strip()
            if site_name:
                curr_lookup[site_name] = status
                # Count ready for enablement circuits
                if 'ready for enablement' in status:
                    ready_count += 1
    
    # Find enablements
    enabled_statuses = ['enabled', 'activated', 'circuit enabled', 'service activated', 'circuit activated', 'live', 'operational', 'in service']
    
    enablements = []
    for site_name, curr_status in curr_lookup.items():
        prev_status = prev_lookup.get(site_name, '')
        
        # Check if became enabled
        is_currently_enabled = any(es in curr_status for es in enabled_statuses)
        was_previously_enabled = any(es in prev_status for es in enabled_statuses)
        
        if is_currently_enabled and not was_previously_enabled:
            # Get full row data
            site_rows = df_curr[df_curr['Site Name'].str.strip() == site_name]
            if not site_rows.empty:
                site_row = site_rows.iloc[0]
                enablements.append({
                    'site_name': site_name,
                    'site_id': safe_str(site_row.get('Site ID', '')),
                    'circuit_purpose': safe_str(site_row.get('Circuit Purpose', '')),
                    'status': curr_status,
                    'previous_status': prev_status,
                    'provider': safe_str(site_row.get('provider_name', '')),
                    'speed': safe_str(site_row.get('details_ordered_service_speed', '')),
                    'cost': safe_str(site_row.get('billing_monthly_cost', '')),
                    'assigned_to': '',  # Will be filled from assignments file
                    'sctask': safe_str(site_row.get('sctask_number', '')),
                    'date': date_str
                })
    
    return {
        'enablements': enablements,
        'ready_count': ready_count,
        'total_circuits': len(df_curr) if df_curr is not None else 0
    }

def process_daily_json_files_for_historical_data(start_date=None, end_date=None, days=None):
    """Process individual daily JSON files to generate complete historical data"""
    try:
        # Find all daily JSON files
        json_pattern = os.path.join(JSON_CACHE_DIR, "daily_tracking_data_*.json")
        json_files = sorted(glob.glob(json_pattern))
        
        # Filter out the sample data file
        json_files = [f for f in json_files if 'sample_data' not in f]
        
        if not json_files:
            logging.warning("No daily JSON files found for historical processing")
            return None
        
        logging.info(f"Found {len(json_files)} daily JSON files for historical processing")
        
        # Filter files by date if specified
        filtered_files = []
        for file_path in json_files:
            try:
                filename = os.path.basename(file_path)
                file_date_str = filename.replace('daily_tracking_data_', '').replace('.json', '')
                file_date = datetime.strptime(file_date_str, '%Y-%m-%d')
                
                # Apply date filters
                include_file = True
                if start_date:
                    start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                    if file_date < start_dt:
                        include_file = False
                
                if end_date:
                    end_dt = datetime.strptime(end_date, '%Y-%m-%d')
                    if file_date > end_dt:
                        include_file = False
                
                if include_file:
                    filtered_files.append((file_path, file_date_str))
                    
            except Exception as e:
                logging.error(f"Error parsing date from {file_path}: {e}")
                continue
        
        # Apply days filter if specified
        if days:
            try:
                days_int = int(days)
                if days_int > 0 and len(filtered_files) > days_int:
                    filtered_files = filtered_files[-days_int:]
            except (ValueError, TypeError):
                pass
        
        logging.info(f"Processing {len(filtered_files)} filtered daily JSON files")
        
        daily_data = []
        all_enablements = []
        daily_attribution = {}
        daily_queue_data = []
        
        # Load assignment data for attribution
        assignment_file = os.path.join(FALLBACK_DATA_DIR, "circuit_assignments.json")
        assignments = load_json_safely(assignment_file, {})
        
        # Process each JSON file
        for i, (file_path, date_str) in enumerate(filtered_files):
            try:
                logging.info(f"Processing {os.path.basename(file_path)} ({i+1}/{len(filtered_files)})")
                
                # Load daily JSON data
                daily_json = load_json_safely(file_path, {})
                if not daily_json:
                    continue
                
                enablement_data = daily_json.get('enablement_data', {})
                enabled_circuits = enablement_data.get('enabled_circuits', [])
                ready_count = enablement_data.get('ready_for_enablement_count', 0)
                total_circuits = daily_json.get('total_circuits', 0)
                
                # Add attribution data to enablements
                enriched_enablements = []
                for enablement in enabled_circuits:
                    site_name = enablement.get('site_name', '')
                    site_id = enablement.get('site_id', '')
                    circuit_purpose = enablement.get('circuit_purpose', '')
                    
                    # Try multiple assignment key formats
                    assignment_keys = [
                        f"{site_name}|{site_id}|{circuit_purpose}",
                        f"{site_name}|{site_name}|{circuit_purpose}",
                        site_name
                    ]
                    
                    assigned_to = 'Unassigned'
                    sctask = ''
                    for key in assignment_keys:
                        if key in assignments:
                            assigned_to = assignments[key].get('assigned_to', 'Unassigned')
                            sctask = assignments[key].get('sctask_number', '')
                            break
                    
                    enriched_enablement = enablement.copy()
                    enriched_enablement['assigned_to'] = assigned_to
                    enriched_enablement['sctask'] = sctask
                    enriched_enablement['date'] = date_str
                    enriched_enablements.append(enriched_enablement)
                
                # Add to daily data with EXACT SAME format as original expects
                daily_data.append({
                    'date': date_str,
                    'count': len(enriched_enablements),
                    'formatted_date': format_date_for_display(date_str),
                    'circuits': enriched_enablements  # For detailed view
                })
                
                # Add to queue data
                daily_queue_data.append({
                    'date': date_str,
                    'formatted_date': format_date_for_display(date_str),
                    'ready_count': ready_count,
                    'closed_from_ready': len(enriched_enablements),
                    'total_circuits': total_circuits
                })
                
                # Track attribution by date
                daily_attribution[date_str] = {
                    'total_enabled': len(enriched_enablements),
                    'attributions': {}
                }
                
                for enablement in enriched_enablements:
                    assigned_to = enablement['assigned_to']
                    if assigned_to not in daily_attribution[date_str]['attributions']:
                        daily_attribution[date_str]['attributions'][assigned_to] = 0
                    daily_attribution[date_str]['attributions'][assigned_to] += 1
                
                # Add to detailed enablements
                all_enablements.extend(enriched_enablements)
                
            except Exception as e:
                logging.error(f"Error processing {file_path}: {e}")
                continue
        
        # Generate summary statistics
        if daily_data:
            total_enabled = sum(item['count'] for item in daily_data)
            avg_per_day = round(total_enabled / len(daily_data), 1) if daily_data else 0
            max_day = max(daily_data, key=lambda x: x['count']) if daily_data else {'count': 0, 'date': ''}
            
            summary = {
                'total_enabled': total_enabled,
                'avg_per_day': avg_per_day,
                'max_day': {
                    'count': max_day['count'],
                    'date': max_day['date']
                },
                'days_analyzed': len(daily_data),
                'period_start': daily_data[0]['date'] if daily_data else '',
                'period_end': daily_data[-1]['date'] if daily_data else '',
                'last_updated': datetime.now().isoformat()
            }
        else:
            summary = {
                'total_enabled': 0,
                'avg_per_day': 0,
                'max_day': {'count': 0, 'date': ''},
                'days_analyzed': 0,
                'period_start': '',
                'period_end': '',
                'last_updated': datetime.now().isoformat()
            }
        
        # Build attribution by person
        attribution_by_person = {}
        for date, data in daily_attribution.items():
            for person, count in data['attributions'].items():
                if person not in attribution_by_person:
                    attribution_by_person[person] = {
                        'total_closures': 0,
                        'dates': {}
                    }
                attribution_by_person[person]['total_closures'] += count
                attribution_by_person[person]['dates'][date] = count
        
        # Format person summary
        person_summary = []
        for person, data in attribution_by_person.items():
            person_summary.append({
                'name': person,
                'total_closures': data['total_closures'],
                'average_per_day': round(data['total_closures'] / len(daily_data), 1) if daily_data else 0,
                'daily_breakdown': data['dates']
            })
        
        # Sort by total closures
        person_summary.sort(key=lambda x: x['total_closures'], reverse=True)
        
        return {
            'daily_data': daily_data,
            'summary': summary,
            'all_enablements': all_enablements,
            'daily_queue_data': daily_queue_data,
            'attribution_by_person': person_summary,
            'attribution_by_date': daily_attribution
        }
        
    except Exception as e:
        logging.error(f"Error in JSON processing: {e}")
        return None

def format_date_for_display(date_str):
    """Format date for chart display (same as original)"""
    try:
        date = datetime.strptime(date_str, '%Y-%m-%d')
        return date.strftime('%b %d')  # "May 4", "Jun 23", etc.
    except:
        return date_str

def format_period_for_display(period_key, period_type):
    """Format period for trend display (same as original)"""
    try:
        if period_type == 'weekly':
            # Convert "2024-W12" to "Week 12, 2024"
            if '-W' in period_key:
                year, week = period_key.split('-W')
                return f"Week {week}, {year}"
            return period_key
        elif period_type == 'monthly':
            # Convert "2024-03" to "Mar 2024"
            if len(period_key) == 7 and '-' in period_key:
                date = datetime.strptime(period_key + '-01', '%Y-%m-%d')
                return date.strftime('%b %Y')
            return period_key
        return period_key
    except:
        return period_key

def filter_data_by_date_range(daily_data, start_date=None, end_date=None, days=None):
    """Filter daily data by date range or number of days (original logic)"""
    if not daily_data:
        return []
    
    filtered_data = daily_data.copy()
    
    if days:
        # Filter by number of days from the end
        try:
            days = int(days)
            if days > 0 and len(filtered_data) > days:
                filtered_data = filtered_data[-days:]
        except (ValueError, TypeError):
            pass
    
    elif start_date and end_date:
        # Filter by date range
        try:
            start_dt = datetime.strptime(start_date, '%Y-%m-%d')
            end_dt = datetime.strptime(end_date, '%Y-%m-%d')
            
            filtered_data = [
                item for item in filtered_data
                if start_dt <= datetime.strptime(item['date'], '%Y-%m-%d') <= end_dt
            ]
        except (ValueError, TypeError) as e:
            logging.error(f"Date filtering error: {e}")
    
    return filtered_data

def recalculate_summary(daily_data):
    """Recalculate summary statistics for filtered data (original logic)"""
    if not daily_data:
        return {
            'total_enabled': 0,
            'avg_per_day': 0,
            'max_day': {'count': 0, 'date': ''},
            'days_analyzed': 0,
            'period_start': '',
            'period_end': '',
            'last_updated': datetime.now().isoformat()
        }
    
    total_enabled = sum(item['count'] for item in daily_data)
    avg_per_day = round(total_enabled / len(daily_data), 1) if daily_data else 0
    max_day = max(daily_data, key=lambda x: x['count'])
    min_date = min(daily_data, key=lambda x: x['date'])['date']
    max_date = max(daily_data, key=lambda x: x['date'])['date']
    
    return {
        'total_enabled': total_enabled,
        'avg_per_day': avg_per_day,
        'max_day': {
            'count': max_day['count'],
            'date': max_day['date']
        },
        'days_analyzed': len(daily_data),
        'period_start': min_date,
        'period_end': max_date,
        'last_updated': datetime.now().isoformat()
    }

def convert_optimized_to_original_format(optimized_data):
    """Convert optimized cache data to original API format"""
    try:
        if not optimized_data or 'daily_summaries' not in optimized_data:
            return {'summary': {}, 'daily_data': []}
        
        daily_summaries = optimized_data['daily_summaries']
        
        # Convert to original daily_data format with formatted_date
        daily_data = []
        for summary in daily_summaries:
            enablement_data = summary.get('enablement_data', {})
            daily_data.append({
                'date': summary['date'],
                'count': enablement_data.get('total_enabled', 0),
                'formatted_date': format_date_for_display(summary['date']),
                'circuits': enablement_data.get('enabled_circuits', [])
            })
        
        # Calculate overall summary
        if daily_data:
            summary = recalculate_summary(daily_data)
        else:
            summary = {
                'total_enabled': 0,
                'avg_per_day': 0,
                'max_day': {'count': 0, 'date': ''},
                'days_analyzed': 0,
                'period_start': '',
                'period_end': '',
                'last_updated': datetime.now().isoformat()
            }
        
        return {
            'summary': summary,
            'daily_data': daily_data
        }
        
    except Exception as e:
        logging.error(f"Error converting optimized data: {e}")
        return {'summary': {}, 'daily_data': []}

@reports_bp.route('/circuit-enablement-report')
def circuit_enablement_report():
    """Circuit Enablement Report page (original)"""
    return render_template('circuit_enablement_report.html')

@reports_bp.route('/api/daily-enablement-data', methods=['GET'])
def daily_enablement_data():
    """
    Get daily enablement statistics (ORIGINAL API CONTRACT)
    
    Query Parameters:
    - days: Number of recent days to include
    - start_date: Start date (YYYY-MM-DD format)
    - end_date: End date (YYYY-MM-DD format)
    """
    try:
        # Try to load optimized data first
        optimized_data = load_json_safely(ENABLEMENT_DATA_FILE)
        
        if optimized_data:
            # Convert optimized data to original format
            base_data = convert_optimized_to_original_format(optimized_data)
            data_source = 'optimized_json_cache'
        else:
            # Fallback to original data format
            base_data = load_json_safely(FALLBACK_ENABLEMENT_FILE, {
                'summary': {
                    'total_enabled': 0,
                    'avg_per_day': 0,
                    'max_day': {'count': 0, 'date': ''},
                    'days_analyzed': 0,
                    'period_start': '',
                    'period_end': '',
                    'last_updated': datetime.now().isoformat()
                },
                'daily_data': []
            })
            data_source = 'fallback_json'
        
        # Get query parameters (original parameter names)
        days = request.args.get('days')
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        
        # Filter data if parameters provided (original logic)
        daily_data = base_data['daily_data']
        if days or (start_date and end_date):
            daily_data = filter_data_by_date_range(daily_data, start_date, end_date, days)
            # Recalculate summary for filtered data
            summary = recalculate_summary(daily_data)
        else:
            summary = base_data['summary']
        
        # Original response format
        response_data = {
            'summary': summary,
            'daily_data': daily_data,
            'data_source': data_source,
            'processing_time': 'fast'
        }
        
        logging.info(f"ðŸ“Š Daily enablement data served: {len(daily_data)} days, {summary['total_enabled']} total enablements")
        
        return jsonify(response_data)
        
    except Exception as e:
        logging.error(f"Error in daily_enablement_data: {e}")
        return jsonify({
            'error': 'Failed to load enablement data',
            'details': str(e),
            'data_source': 'error'
        }), 500

@reports_bp.route('/api/enablement-trend', methods=['GET'])
def enablement_trend():
    """
    Get enablement trend data (ORIGINAL API CONTRACT)
    
    Query Parameters:
    - period: 'weekly' or 'monthly' (original parameter name)
    """
    try:
        period = request.args.get('period', 'weekly')  # Original parameter name
        
        # Try optimized data first
        optimized_trend_data = load_json_safely(TREND_DATA_FILE)
        
        if optimized_trend_data:
            # Convert optimized trend data to original format
            if period == 'monthly':
                raw_trend = optimized_trend_data.get('monthly', {})
            else:
                raw_trend = optimized_trend_data.get('weekly', {})
            
            # Convert to original format with formatted_period
            trend_result = []
            for period_key, period_data in raw_trend.items():
                if isinstance(period_data, dict):
                    trend_result.append({
                        'period': period_key,
                        'count': period_data.get('total_enabled', 0),
                        'formatted_period': format_period_for_display(period_key, period),
                        'avg_per_day': period_data.get('avg_per_day', 0),
                        'days_in_period': len(period_data.get('days', []))
                    })
            
            # Sort by period
            trend_result.sort(key=lambda x: x['period'])
            data_source = 'optimized_json_cache'
            
        else:
            # Fallback to original format
            fallback_trend = load_json_safely(FALLBACK_TREND_FILE, {
                'weekly': [],
                'monthly': [],
                'last_updated': datetime.now().isoformat()
            })
            
            if period == 'monthly':
                trend_result = fallback_trend.get('monthly', [])
            else:
                trend_result = fallback_trend.get('weekly', [])
                
            data_source = 'fallback_json'
        
        # Original response format
        response_data = {
            'trend_data': trend_result,  # Original field name
            'period': period,
            'data_source': data_source,
            'last_updated': datetime.now().isoformat()
        }
        
        logging.info(f"ðŸ“ˆ Trend data served: {period}, {len(trend_result)} periods")
        
        return jsonify(response_data)
        
    except Exception as e:
        logging.error(f"Error in enablement_trend: {e}")
        return jsonify({
            'error': 'Failed to load trend data',
            'details': str(e),
            'data_source': 'error'
        }), 500

@reports_bp.route('/api/enablement-details', methods=['GET'])
def enablement_details():
    """
    Get detailed enablement data with circuit information and assignments (ORIGINAL API)
    
    Query Parameters:
    - date: Specific date (YYYY-MM-DD format)
    - start_date: Start date for range
    - end_date: End date for range
    - days: Number of recent days
    """
    try:
        # Try optimized data first
        optimized_data = load_json_safely(ENABLEMENT_DATA_FILE)
        
        if optimized_data and 'daily_summaries' in optimized_data:
            # Extract detailed enablements from optimized data
            enablements = []
            for summary in optimized_data['daily_summaries']:
                enablement_data = summary.get('enablement_data', {})
                enabled_circuits = enablement_data.get('enabled_circuits', [])
                
                for circuit in enabled_circuits:
                    enablements.append({
                        'date': summary['date'],
                        'site_name': circuit.get('site_name', ''),
                        'site_id': circuit.get('site_id', ''),
                        'circuit_purpose': circuit.get('circuit_purpose', ''),
                        'status': circuit.get('status', ''),
                        'previous_status': circuit.get('previous_status', ''),
                        'provider': circuit.get('provider', ''),
                        'speed': circuit.get('speed', ''),
                        'cost': circuit.get('cost', ''),
                        'assigned_to': circuit.get('assigned_to', ''),
                        'sctask': circuit.get('sctask', '')
                    })
            
            data_source = 'optimized_json_cache'
        else:
            # Fallback to original detailed data
            detailed_data = load_json_safely(FALLBACK_DETAILED_FILE, {
                'enablements': [],
                'total_count': 0,
                'last_updated': datetime.now().isoformat()
            })
            enablements = detailed_data.get('enablements', [])
            data_source = 'fallback_json'
        
        # Get query parameters (original)
        specific_date = request.args.get('date')
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        days = request.args.get('days')
        
        # Filter enablements (original logic)
        filtered_enablements = enablements
        
        if specific_date:
            filtered_enablements = [e for e in enablements if e['date'] == specific_date]
        elif start_date and end_date:
            try:
                start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                end_dt = datetime.strptime(end_date, '%Y-%m-%d')
                filtered_enablements = [
                    e for e in enablements
                    if start_dt <= datetime.strptime(e['date'], '%Y-%m-%d') <= end_dt
                ]
            except (ValueError, TypeError):
                pass
        elif days:
            try:
                days_int = int(days)
                cutoff_date = datetime.now() - timedelta(days=days_int)
                filtered_enablements = [
                    e for e in enablements
                    if datetime.strptime(e['date'], '%Y-%m-%d') >= cutoff_date
                ]
            except (ValueError, TypeError):
                pass
        
        # Original response format
        response_data = {
            'enablements': filtered_enablements,
            'total_count': len(filtered_enablements),
            'filtered_from': len(enablements),
            'data_source': data_source,
            'last_updated': datetime.now().isoformat()
        }
        
        logging.info(f"ðŸ“‹ Detailed enablements served: {len(filtered_enablements)} records")
        
        return jsonify(response_data)
        
    except Exception as e:
        logging.error(f"Error in enablement_details: {e}")
        return jsonify({
            'error': 'Failed to load detailed enablement data',
            'details': str(e),
            'data_source': 'error'
        }), 500

@reports_bp.route('/api/enablement-data-status', methods=['GET'])
def enablement_data_status():
    """Get status of enablement data files (original + optimized)"""
    try:
        status = {}
        
        # Check optimized files
        optimized_files = {
            'optimized_master': ENABLEMENT_DATA_FILE,
            'optimized_trends': TREND_DATA_FILE,
            'optimized_stats': QUICK_STATS_FILE
        }
        
        # Check original fallback files
        fallback_files = {
            'fallback_daily': FALLBACK_ENABLEMENT_FILE,
            'fallback_trends': FALLBACK_TREND_FILE,
            'fallback_detailed': FALLBACK_DETAILED_FILE
        }
        
        all_files = {**optimized_files, **fallback_files}
        
        for name, file_path in all_files.items():
            if os.path.exists(file_path):
                stat = os.stat(file_path)
                modified_time = datetime.fromtimestamp(stat.st_mtime)
                
                status[name] = {
                    'exists': True,
                    'size_bytes': stat.st_size,
                    'size_mb': round(stat.st_size / (1024 * 1024), 2),
                    'last_modified': modified_time.isoformat(),
                    'age_hours': round((datetime.now() - modified_time).total_seconds() / 3600, 1)
                }
            else:
                status[name] = {'exists': False, 'error': 'File not found'}
        
        # Determine performance mode
        has_optimized = status.get('optimized_master', {}).get('exists', False)
        has_fallback = status.get('fallback_daily', {}).get('exists', False)
        
        if has_optimized:
            performance_mode = 'optimized'
            primary_source = 'optimized_json_cache'
        elif has_fallback:
            performance_mode = 'fallback'
            primary_source = 'fallback_json'
        else:
            performance_mode = 'unavailable'
            primary_source = 'none'
        
        response_data = {
            'performance_mode': performance_mode,
            'primary_source': primary_source,
            'files': status,
            'last_checked': datetime.now().isoformat()
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        logging.error(f"Error checking data status: {e}")
        return jsonify({
            'error': str(e),
            'last_checked': datetime.now().isoformat()
        }), 500

@reports_bp.route('/api/ready-queue-data', methods=['GET'])
def ready_queue_data():
    """
    Get daily ready queue statistics with CSV FALLBACK for full historical data
    
    Tries JSON cache first, falls back to CSV processing for complete history.
    
    Query Parameters:
        days (int): Number of days to analyze 
        start_date (str): Start date in YYYY-MM-DD format
        end_date (str): End date in YYYY-MM-DD format
        (no parameters = all available data)
    
    Returns:
        JSON response with daily ready queue data
    """
    try:
        # Get parameters
        days = request.args.get('days', type=int)
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        use_csv_fallback = False
        
        # Try JSON cache first
        master_data = load_json_safely(ENABLEMENT_DATA_FILE, {})
        
        # Check if JSON cache has sufficient data
        if master_data and 'daily_summaries' in master_data:
            json_start_date = master_data.get('date_range', {}).get('start', '')
            
            # Determine if we need CSV fallback
            if not days and not start_date and not end_date:
                # User wants ALL data - check if JSON has complete history
                use_csv_fallback = True  # Always use CSV for complete history
                logging.info("Using CSV fallback for complete historical data")
            elif start_date:
                # Check if requested start date is before JSON cache starts
                try:
                    req_start = datetime.strptime(start_date, '%Y-%m-%d')
                    json_start = datetime.strptime(json_start_date, '%Y-%m-%d') if json_start_date else datetime.now()
                    if req_start < json_start:
                        use_csv_fallback = True
                        logging.info(f"Using CSV fallback: requested start {start_date} before JSON start {json_start_date}")
                except:
                    pass
        else:
            use_csv_fallback = True
            logging.info("No JSON cache available - using CSV fallback")
        
        if use_csv_fallback:
            # Use daily JSON files for complete historical data
            logging.info("Processing daily JSON files for ready queue data")
            json_data = process_daily_json_files_for_historical_data(start_date, end_date, days)
            
            if json_data:
                queue_data = json_data['daily_queue_data']
                
                # Calculate summary statistics
                total_ready = sum(d.get('ready_count', 0) for d in queue_data)
                avg_queue_size = total_ready / len(queue_data) if queue_data else 0
                total_closed = sum(d.get('closed_from_ready', 0) for d in queue_data)
                
                return jsonify({
                    'success': True,
                    'data': queue_data,
                    'summary': {
                        'average_queue_size': round(avg_queue_size, 1),
                        'total_ready': total_ready,
                        'total_closed_from_ready': total_closed,
                        'days_analyzed': len(queue_data)
                    },
                    'data_source': 'daily_json_historical_processing',
                    'generated_at': datetime.now().isoformat()
                })
            else:
                return jsonify({
                    'error': 'No historical data available from daily JSON processing',
                    'success': False
                }), 404
        
        else:
            # Use optimized JSON cache
            queue_data = []
            for daily_summary in master_data['daily_summaries']:
                if 'enablement_data' in daily_summary:
                    enablement = daily_summary['enablement_data']
                    queue_data.append({
                        'date': enablement['date'],
                        'formatted_date': format_date_for_display(enablement['date']),
                        'ready_count': enablement.get('ready_for_enablement_count', 0),
                        'closed_from_ready': enablement.get('total_enabled', 0),
                        'total_circuits': daily_summary.get('total_circuits', 0)
                    })
            
            # Sort by date
            queue_data.sort(key=lambda x: x['date'])
            
            # Filter by date range
            filtered_data = filter_data_by_date_range(queue_data, start_date, end_date, days)
            
            # Calculate summary statistics
            total_ready = sum(d.get('ready_count', 0) for d in filtered_data)
            avg_queue_size = total_ready / len(filtered_data) if filtered_data else 0
            total_closed = sum(d.get('closed_from_ready', 0) for d in filtered_data)
            
            return jsonify({
                'success': True,
                'data': filtered_data,
                'summary': {
                    'average_queue_size': round(avg_queue_size, 1),
                    'total_ready': total_ready,
                    'total_closed_from_ready': total_closed,
                    'days_analyzed': len(filtered_data)
                },
                'data_source': 'optimized_json',
                'generated_at': datetime.now().isoformat()
            })
        
    except Exception as e:
        logging.error(f"Error in ready_queue_data: {e}")
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

@reports_bp.route('/api/closure-attribution-data', methods=['GET'])
def closure_attribution_data():
    """
    Get closure data with assignment attribution - with daily JSON fallback for complete history
    
    Analyzes enablements with assignment data to show who closed 
    which tickets and team performance metrics.
    
    Query Parameters:
        days (int): Number of days to analyze
        start_date (str): Start date in YYYY-MM-DD format
        end_date (str): End date in YYYY-MM-DD format
        (no parameters = all available data)
    
    Returns:
        JSON response with closure attribution data
    """
    try:
        # Get parameters
        days = request.args.get('days', type=int)
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        use_daily_json_fallback = False
        
        # Try optimized JSON cache first
        master_data = load_json_safely(ENABLEMENT_DATA_FILE, {})
        
        # Check if JSON cache has sufficient data
        if master_data and 'daily_summaries' in master_data:
            json_start_date = master_data.get('date_range', {}).get('start', '')
            
            # Determine if we need daily JSON fallback
            if not days and not start_date and not end_date:
                # User wants ALL data - use daily JSON files for complete history
                use_daily_json_fallback = True
                logging.info("Using daily JSON fallback for complete attribution history")
            elif start_date:
                # Check if requested start date is before JSON cache starts
                try:
                    req_start = datetime.strptime(start_date, '%Y-%m-%d')
                    json_start = datetime.strptime(json_start_date, '%Y-%m-%d') if json_start_date else datetime.now()
                    if req_start < json_start:
                        use_daily_json_fallback = True
                        logging.info(f"Using daily JSON fallback: requested start {start_date} before JSON start {json_start_date}")
                except:
                    pass
        else:
            use_daily_json_fallback = True
            logging.info("No JSON cache available - using daily JSON fallback")
        
        if use_daily_json_fallback:
            # Use daily JSON files for complete historical attribution data
            logging.info("Processing daily JSON files for closure attribution data")
            json_data = process_daily_json_files_for_historical_data(start_date, end_date, days)
            
            if json_data:
                return jsonify({
                    'success': True,
                    'attribution_by_person': json_data['attribution_by_person'],
                    'attribution_by_date': json_data['attribution_by_date'],
                    'summary': {
                        'total_team_members': len(json_data['attribution_by_person']),
                        'total_attributed': sum(p['total_closures'] for p in json_data['attribution_by_person']),
                        'days_analyzed': len(json_data['daily_data'])
                    },
                    'data_source': 'daily_json_historical_processing',
                    'generated_at': datetime.now().isoformat()
                })
            else:
                return jsonify({
                    'error': 'No historical attribution data available from daily JSON processing',
                    'success': False
                }), 404
        
        else:
            # Use optimized JSON cache for recent data
            assignment_file = os.path.join(FALLBACK_DATA_DIR, "circuit_assignments.json")
            assignments = load_json_safely(assignment_file, {})
            
            # Extract enablement data and build date filter
            daily_data = []
            for daily_summary in master_data['daily_summaries']:
                if 'enablement_data' in daily_summary:
                    enablement = daily_summary['enablement_data']
                    daily_data.append({
                        'date': enablement['date'],
                        'count': enablement.get('total_enabled', 0),
                        'enabled_circuits': enablement.get('enabled_circuits', [])
                    })
            
            # Sort by date
            daily_data.sort(key=lambda x: x['date'])
            
            # Filter by date range
            filtered_data = filter_data_by_date_range(daily_data, start_date, end_date, days)
            
            # Build attribution data
            attribution_by_person = {}
            attribution_by_date = {}
            
            for day in filtered_data:
                date = day['date']
                attribution_by_date[date] = {
                    'total_enabled': day.get('count', 0),
                    'attributions': {}
                }
                
                # Process enabled circuits for this day
                enabled_circuits = day.get('enabled_circuits', [])
                
                for circuit in enabled_circuits:
                    site_name = circuit.get('site_name', '')
                    site_id = circuit.get('site_id', '')
                    circuit_purpose = circuit.get('circuit_purpose', '')
                    
                    # Try multiple assignment key formats
                    assignment_keys = [
                        f"{site_name}|{site_id}|{circuit_purpose}",
                        f"{site_name}|{site_name}|{circuit_purpose}",
                        site_name
                    ]
                    
                    assigned_to = 'Unassigned'
                    for key in assignment_keys:
                        if key in assignments:
                            assigned_to = assignments[key].get('assigned_to', 'Unassigned')
                            break
                    
                    # Track by person
                    if assigned_to not in attribution_by_person:
                        attribution_by_person[assigned_to] = {
                            'total_closures': 0,
                            'dates': {}
                        }
                    
                    attribution_by_person[assigned_to]['total_closures'] += 1
                    
                    if date not in attribution_by_person[assigned_to]['dates']:
                        attribution_by_person[assigned_to]['dates'][date] = 0
                    attribution_by_person[assigned_to]['dates'][date] += 1
                    
                    # Track by date
                    if assigned_to not in attribution_by_date[date]['attributions']:
                        attribution_by_date[date]['attributions'][assigned_to] = 0
                    attribution_by_date[date]['attributions'][assigned_to] += 1
            
            # Format response
            person_summary = []
            for person, data in attribution_by_person.items():
                person_summary.append({
                    'name': person,
                    'total_closures': data['total_closures'],
                    'average_per_day': round(data['total_closures'] / len(filtered_data), 1) if filtered_data else 0,
                    'daily_breakdown': data['dates']
                })
            
            # Sort by total closures
            person_summary.sort(key=lambda x: x['total_closures'], reverse=True)
            
            return jsonify({
                'success': True,
                'attribution_by_person': person_summary,
                'attribution_by_date': attribution_by_date,
                'summary': {
                    'total_team_members': len(attribution_by_person),
                    'total_attributed': sum(p['total_closures'] for p in person_summary),
                    'days_analyzed': len(filtered_data)
                },
                'data_source': 'optimized_json',
                'generated_at': datetime.now().isoformat()
            })
        
    except Exception as e:
        logging.error(f"Error in closure_attribution_data: {e}")
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

@reports_bp.route('/api/reports-health', methods=['GET'])
def reports_health():
    """
    Check reports system health and performance mode
    
    Returns:
        JSON response with system health status
    """
    try:
        # Check which data files exist
        optimized_exists = os.path.exists(ENABLEMENT_DATA_FILE)
        fallback_exists = os.path.exists(FALLBACK_ENABLEMENT_FILE)
        
        # Determine performance mode
        if optimized_exists:
            performance_mode = 'optimized'
            data_source = 'json_cache'
        elif fallback_exists:
            performance_mode = 'csv_fallback'
            data_source = 'csv_processing'
        else:
            performance_mode = 'degraded'
            data_source = 'none'
        
        # Get file timestamps
        file_stats = {}
        if optimized_exists:
            stat = os.stat(ENABLEMENT_DATA_FILE)
            file_stats['optimized_cache'] = {
                'exists': True,
                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'size': stat.st_size
            }
        
        if fallback_exists:
            stat = os.stat(FALLBACK_ENABLEMENT_FILE)
            file_stats['fallback_data'] = {
                'exists': True,
                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'size': stat.st_size
            }
        
        return jsonify({
            'status': 'healthy' if performance_mode != 'degraded' else 'degraded',
            'performance_mode': performance_mode,
            'data_source': data_source,
            'file_stats': file_stats,
            'endpoints_available': [
                'daily-enablement-data',
                'enablement-trend',
                'ready-queue-data',
                'closure-attribution-data'
            ],
            'checked_at': datetime.now().isoformat()
        })
        
    except Exception as e:
        logging.error(f"Error in reports_health: {e}")
        return jsonify({
            'status': 'error',
            'performance_mode': 'degraded',
            'error': str(e),
            'checked_at': datetime.now().isoformat()
        }), 500

# Legacy endpoint support (original)
@reports_bp.route('/api/legacy-enablement-data', methods=['GET'])
def legacy_enablement_data():
    """Legacy endpoint that maintains compatibility (original)"""
    try:
        return daily_enablement_data()
    except Exception as e:
        return jsonify({
            'error': 'Legacy endpoint error',
            'details': str(e)
        }), 500

# Module info (original)
def get_module_info():
    """Return module information for debugging (original)"""
    return {
        'module': 'reports.py',
        'version': '2.1.0',
        'performance_mode': 'optimized_with_fallback',
        'primary_data_files': {
            'optimized_master': ENABLEMENT_DATA_FILE,
            'optimized_trends': TREND_DATA_FILE,
            'optimized_stats': QUICK_STATS_FILE
        },
        'fallback_data_files': {
            'fallback_daily': FALLBACK_ENABLEMENT_FILE,
            'fallback_trends': FALLBACK_TREND_FILE,
            'fallback_detailed': FALLBACK_DETAILED_FILE
        },
        'endpoints': [
            '/circuit-enablement-report',
            '/api/daily-enablement-data',
            '/api/enablement-trend',
            '/api/enablement-details',
            '/api/enablement-data-status',
            '/api/ready-queue-data',
            '/api/closure-attribution-data',
            '/api/reports-health'
        ]
    }

if __name__ == "__main__":
    # Module test
    print("Reports module info:")
    import pprint
    pprint.pprint(get_module_info())